{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils.validation import check_is_fitted, check_array , check_X_y\n",
    "from sklearn.base import clone\n",
    "from sklearn.exceptions import NotFittedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.Initialization\\n- initialize weights for all instances to be equal : 1/m\\n- empty lists to store alphas and classifiers\\n\\n2.fit() method\\n- train weak classfier using current weights(alphas)\\n- calculate weighted errors \\n- calculate alphas based on error rate\\n- update weights of the instances for the next iteration\\n- normalize weights\\n- store trained classifiers and alphas\\n\\n3. predict() method\\n- aggregate predictions of all classifiers using their alphas\\n- return final prediction as sign of the sum of weighted predictions\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1.Initialization\n",
    "- initialize weights for all instances to be equal : 1/m\n",
    "- empty lists to store alphas and classifiers\n",
    "\n",
    "2.fit() method\n",
    "- train weak classfier using current weights(alphas)\n",
    "- calculate weighted errors \n",
    "- calculate alphas based on error rate\n",
    "- update weights of the instances for the next iteration\n",
    "- normalize weights\n",
    "- store trained classifiers and alphas\n",
    "\n",
    "3. predict() method\n",
    "- aggregate predictions of all classifiers using their alphas\n",
    "- return final prediction as sign of the sum of weighted predictions\n",
    "\n",
    "4. predict_proba() method\n",
    "- return probability of the final prediction as sum of weighted probabilities\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adab():\n",
    "    def __init__(self, weak_classifier, n_classifiers,learning_rate=1.0):\n",
    "        self.weak_classifier = weak_classifier #weak classifier to be used\n",
    "        self.n_classifiers = n_classifiers\n",
    "        self.alphas = []\n",
    "        self.classifiers = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.proba = None\n",
    "        self.is_fitted_ = False\n",
    "    def fit(self,X,y):\n",
    "\n",
    "        #first check if X and y have the same length\n",
    "        X,y = check_X_y(X,y)\n",
    "\n",
    "        #initialize weights\n",
    "        m,n = X.shape\n",
    "        w = np.ones(m) / m\n",
    "        self.alphas = []\n",
    "        self.classifiers = []\n",
    "\n",
    "        for t in range(self.n_classifiers):\n",
    "            clf = self.weak_classifier()\n",
    "            clf.fit(X,y,sample_weight=w)\n",
    "            y_pred = clf.predict(X)\n",
    "\n",
    "            # 1.weighted error rates\n",
    "            # error = sum of weights of misclassified instances\n",
    "            r = np.sum(w*(y_pred != y))\n",
    "\n",
    "            # 2. predictor weights (alpha)\n",
    "            # alpha = learning_rate * ln((1-error)/error)\n",
    "            # to avoid division by zero, add a small value to error\n",
    "            alpha = self.learning_rate * np.log((1-r)/(r+1e-10))\n",
    "            #store alpha\n",
    "            self.alphas.append(alpha)\n",
    "            #store classifier\n",
    "            self.classifiers.append(clf)\n",
    "\n",
    "            # 3. update weights\n",
    "            # increase weights of misclassified instances\n",
    "            # decrease weights of correctly classified instances\n",
    "            w = w * np.exp(alpha * (y_pred != y))\n",
    "            # normalize weights\n",
    "            w = w / np.sum(w)\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "\n",
    "        # fit method always returns self\n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        # check if fitted \n",
    "        check_is_fitted(self,'is_fitted_')\n",
    "\n",
    "        # validate X input\n",
    "        X = check_array(X)\n",
    "\n",
    "        #get predictions from all stored weak_classifiers\n",
    "        clf_preds = np.array([alpha * clf.predict(X) for alpha,clf in zip(self.alphas, self.classifiers)])\n",
    "        #return sign of sum of weighted predictions\n",
    "        y_pred = np.sign(np.sum(clf_preds,axis=0))\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        # check if fitted \n",
    "        check_is_fitted(self,'is_fitted_')\n",
    "\n",
    "        # validate X input\n",
    "        X = check_array(X)\n",
    "\n",
    "        #get predictions from all stored weak_classifiers\n",
    "        clf_preds = np.array([alpha * clf.predict(X) for alpha,clf in zip(self.alphas, self.classifiers)])\n",
    "        #return sign of sum of weighted predictions\n",
    "        proba = np.sum(clf_preds,axis=0)\n",
    "\n",
    "        proba = (proba - proba.min())/proba.ptp()\n",
    "        return np.vstack([1-proba,proba]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 20), (1000,))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = make_classification(n_samples=1000,n_features=20,n_informative=10,n_classes=2,random_state=42)\n",
    "\n",
    "X.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 20) (200, 20) (800,) (200,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = adab(weak_classifier= lambda: DecisionTreeClassifier(random_state=42,\n",
    "                                                            max_depth=20,\n",
    "                                                            splitter='random',\n",
    "                                                            criterion='entropy'),\n",
    "\n",
    "            n_classifiers=1000,\n",
    "            learning_rate=0.5)\n",
    "\n",
    "clf1.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf1.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.classifiers[50].get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 1.   ],\n",
       "       [1.   , 0.   ],\n",
       "       [1.   , 0.   ],\n",
       "       [1.   , 0.   ],\n",
       "       [1.   , 0.   ],\n",
       "       [0.   , 1.   ],\n",
       "       [1.   , 0.   ],\n",
       "       [1.   , 0.   ],\n",
       "       [1.   , 0.   ],\n",
       "       [0.323, 0.677],\n",
       "       [0.   , 1.   ],\n",
       "       [0.   , 1.   ],\n",
       "       [1.   , 0.   ],\n",
       "       [0.   , 1.   ],\n",
       "       [0.   , 1.   ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = clf1.predict_proba(X_test)\n",
    "y_proba[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.512925464970229,\n",
       " 11.512925464970229,\n",
       " 11.512925464970229,\n",
       " 11.512925464970229,\n",
       " 11.512925464970229,\n",
       " 11.512925464970229,\n",
       " 11.512925464970229,\n",
       " 11.512925464970229,\n",
       " 11.512925464970229,\n",
       " 11.512925464970229]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.alphas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aarad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.845"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "                            n_estimators=1000,\n",
    "                            learning_rate=0.5)\n",
    "\n",
    "clf2.fit(X_train,y_train)\n",
    "\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
